1
00:00:00,000 --> 00:00:10,440
One of the most common complex applications that people are building using an LLM is a
人们使用LLM构建的最常见的复杂应用之一是一个

2
00:00:10,440 --> 00:00:15,240
system that can answer questions on top of or about a document.
可以回答关于文件上面的问题或关于文件的问题的系统。

3
00:00:15,240 --> 00:00:20,600
So given a piece of text may be extracted from PDF file or from a webpage or from some
因此，给定一个文本可以从PDF文件或网页中提取，也可以从一些其他文件中提取。

4
00:00:20,600 --> 00:00:26,920
company's intranet internal document collection, can you use an LLM to answer questions about
公司内部网的内部文件集，你能不能用法律硕士来回答关于

5
00:00:26,920 --> 00:00:31,400
the content of those documents to help users gain a deeper understanding and get access
这些文件的内容，以帮助用户获得更深入的理解和获得

6
00:00:31,400 --> 00:00:33,480
to the information that they need?
他们需要的信息？

7
00:00:33,480 --> 00:00:37,440
This is really powerful because it starts to combine these language models with data
这真的很强大，因为它开始将这些语言模型与数据相结合

8
00:00:37,440 --> 00:00:39,300
that they weren't originally trained on.
他们最初没有接受过培训。

9
00:00:39,300 --> 00:00:42,540
So it makes them much more flexible and adaptable to your use case.
因此，它使它们更加灵活，并能适应你的使用情况。

10
00:00:42,540 --> 00:00:47,080
It's also really exciting because we'll start to move beyond language models, prompts, and
这也是非常令人兴奋的，因为我们将开始超越语言模型、提示和

11
00:00:47,080 --> 00:00:51,700
output parsers and start introducing some more of the key components of linkchain, such
输出分析器，并开始介绍链接链的一些更多的关键组成部分，如

12
00:00:51,700 --> 00:00:54,600
as embedding models and vector stores.
作为嵌入模型和矢量存储。

13
00:00:54,600 --> 00:00:57,640
As Andrew mentioned, this is one of the more popular chains that we've got, so I hope you're
正如安德鲁提到的，这是我们得到的较受欢迎的连锁店之一，所以我希望你

14
00:00:57,640 --> 00:00:58,640
excited.
兴奋。

15
00:00:58,640 --> 00:01:03,580
In fact, embeddings and vector stores are some of the most powerful modern techniques.
事实上，嵌入和矢量存储是一些最强大的现代技术。

16
00:01:03,580 --> 00:01:08,320
So if you have not seen them yet, they are very much worth learning about.
因此，如果你还没有看到它们，它们是非常值得学习的。

17
00:01:08,320 --> 00:01:10,080
So with that, let's dive in.
因此，在这一点上，让我们深入了解。

18
00:01:10,080 --> 00:01:11,800
Let's do it.
让我们行动起来吧。

19
00:01:11,800 --> 00:01:16,280
So we're going to start by importing the environment variables as we always do.
因此，我们将像往常一样，从导入环境变量开始。

20
00:01:16,280 --> 00:01:20,080
Now we're going to import some things that will help us when building this chain.
现在我们要导入一些东西，这些东西将在建立这个链条时帮助我们。

21
00:01:20,080 --> 00:01:22,000
We're going to import the retrieval QA chain.
我们要导入检索的QA链。

22
00:01:22,000 --> 00:01:24,360
This will do retrieval over some documents.
这将在一些文件上做检索。

23
00:01:24,360 --> 00:01:28,120
We're going to import our favorite chat open AI language model.
我们要导入我们最喜欢的聊天开放式AI语言模型。

24
00:01:28,120 --> 00:01:29,720
We're going to import a document loader.
我们要导入一个文档加载器。

25
00:01:29,720 --> 00:01:33,480
This is going to be used to load some proprietary data that we're going to combine with the
这将被用来加载一些专有数据，我们将把这些数据与

26
00:01:33,480 --> 00:01:34,480
language model.
语言模式。

27
00:01:34,480 --> 00:01:36,380
In this case, it's going to be in a CSV.
在这种情况下，它将是一个CSV格式。

28
00:01:36,380 --> 00:01:38,400
So we're going to import the CSV loader.
所以我们要导入CSV加载器。

29
00:01:38,400 --> 00:01:41,240
Finally, we're going to import a vector store.
最后，我们要导入一个矢量存储。

30
00:01:41,240 --> 00:01:44,720
There are many different types of vector stores, and we'll cover what exactly these are later
有许多不同类型的矢量商店，我们将在后面介绍这些商店的具体内容

31
00:01:44,720 --> 00:01:48,920
on, but we're going to get started with the Docker array in-memory search vector store.
上，但我们要开始使用Docker阵列的内存搜索向量存储。

32
00:01:48,920 --> 00:01:52,860
This is really nice because it's an in-memory vector store, and it doesn't require connecting
这真的很好，因为它是一个内存向量存储，而且它不需要连接

33
00:01:52,860 --> 00:01:57,560
to an external database of any kind, so it makes it really easy to get started.
到任何类型的外部数据库，所以它使人们非常容易上手。

34
00:01:57,560 --> 00:02:02,480
We're also going to import display and markdown, two common utilities for displaying information
我们还将导入display和markdown，这是两个用于显示信息的常用工具

35
00:02:02,480 --> 00:02:04,520
in Jupyter notebooks.
在Jupyter笔记本中。

36
00:02:04,520 --> 00:02:09,480
We've provided a CSV of outdoor clothing that we're going to use to combine with the language
我们提供了一个户外服装的CSV，我们要用它来与语言相结合

37
00:02:09,480 --> 00:02:10,480
model.
模型。

38
00:02:10,480 --> 00:02:18,160
Here, we're going to initialize a loader, the CSV loader, with a path to this file.
在这里，我们要初始化一个加载器，即CSV加载器，并给出这个文件的路径。

39
00:02:18,160 --> 00:02:22,800
We're next going to import an index, the vector store index creator.
我们接下来要导入一个索引，即矢量存储索引创建者。

40
00:02:22,800 --> 00:02:25,980
This will help us create a vector store really easily.
这将帮助我们真正轻松地创建一个矢量商店。

41
00:02:25,980 --> 00:02:33,920
As we can see below, it'll only be a few lines of code to create this.
正如我们在下面看到的，只需要几行代码就可以创建这个。

42
00:02:33,920 --> 00:02:37,120
To create it, we're going to specify two things.
为了创建它，我们要指定两件事。

43
00:02:37,120 --> 00:02:40,560
First, we're going to specify the vector store class.
首先，我们要指定向量存储类。

44
00:02:40,560 --> 00:02:44,240
As mentioned before, we're going to use this vector store as it's a particularly easy one
如前所述，我们将使用这个矢量存储，因为它是一个特别简单的存储。

45
00:02:44,240 --> 00:02:46,100
to get started with.
以此为起点。

46
00:02:46,100 --> 00:02:50,200
After it's been created, we're then going to call from loaders, which takes in a list
在它被创建后，我们将从加载器中调用，它接收了一个列表

47
00:02:50,200 --> 00:02:51,840
of document loaders.
的文件加载器。

48
00:02:51,840 --> 00:02:58,480
We've only got one loader that we really care about, so that's what we're passing in here.
我们只有一台我们真正关心的装载机，所以这就是我们在这里传递的东西。

49
00:02:58,480 --> 00:03:02,240
It's now been created, and we can start to ask questions about it.
它现在已经被创造出来了，我们可以开始对它提出问题。

50
00:03:02,240 --> 00:03:05,920
Below we'll cover what exactly happened under the hood, so let's not worry about that for
下面我们将介绍引擎盖下到底发生了什么，所以我们不要担心这个问题。

51
00:03:05,920 --> 00:03:06,920
now.
现在。

52
00:03:06,920 --> 00:03:09,160
Here, we'll start with a query.
在这里，我们将从一个查询开始。

53
00:03:09,160 --> 00:03:16,360
We'll then create a response using index query and pass in this query.
然后我们将使用索引查询创建一个响应，并传入这个查询。

54
00:03:16,360 --> 00:03:20,900
Again, we'll cover what's going on under the hood down below.
同样，我们将在下面介绍引擎盖下发生的事情。

55
00:03:20,900 --> 00:03:30,380
For now, we'll just wait for it to respond.
现在，我们只是等待它的回应。

56
00:03:30,380 --> 00:03:34,520
After it finishes, we can now take a look at what exactly was returned.
完成后，我们现在可以看一下到底返回了什么。

57
00:03:34,520 --> 00:03:39,760
We've gotten back a table in Markdown with names and descriptions for all shirts with
我们已经在Markdown中找回了一个表格，上面有所有衬衫的名称和描述，其中有

58
00:03:39,760 --> 00:03:40,760
sun protection.
防晒。

59
00:03:40,760 --> 00:03:45,560
We've also got a nice little summary that the language model has provided us.
我们还得到了语言模型为我们提供的一个漂亮的小摘要。

60
00:03:45,560 --> 00:03:49,320
So we've gone over how to do question answering over your documents, but what exactly is going
因此，我们已经讨论了如何在你的文件上做问题回答，但到底是什么呢？

61
00:03:49,320 --> 00:03:52,160
on underneath the hood?
在引擎盖下面？

62
00:03:52,160 --> 00:03:54,320
First let's think about the general idea.
首先让我们思考一下总体思路。

63
00:03:54,320 --> 00:03:58,800
We want to use language models and combine it with a lot of our documents, but there's
我们想使用语言模型，并将其与我们的许多文件结合起来，但有

64
00:03:58,800 --> 00:03:59,880
a key issue.
一个关键问题。

65
00:03:59,880 --> 00:04:03,440
Language models can only inspect a few thousand words at a time.
语言模型一次只能检查几千个单词。

66
00:04:03,440 --> 00:04:07,760
So if we have really large documents, how can we get the language model to answer questions
因此，如果我们有非常大的文件，我们如何能让语言模型回答问题？

67
00:04:07,760 --> 00:04:10,280
about everything that's in there?
关于里面的一切？

68
00:04:10,280 --> 00:04:14,440
This is where embeddings and vector stores come into play.
这就是嵌入和矢量存储发挥作用的地方。

69
00:04:14,440 --> 00:04:17,760
First let's talk about embeddings.
首先让我们来谈谈嵌入。

70
00:04:17,760 --> 00:04:21,720
Embeddings create numerical representations for pieces of text.
嵌入法为文本片段创建了数字表示。

71
00:04:21,720 --> 00:04:26,040
This numerical representation captures the semantic meaning of the piece of text that
这种数字表示法抓住了这段文字的语义。

72
00:04:26,040 --> 00:04:28,000
it's been run over.
它已经被碾压。

73
00:04:28,000 --> 00:04:31,920
Pieces of text with similar content will have similar vectors.
内容相似的文本片断会有相似的矢量。

74
00:04:31,920 --> 00:04:35,280
This lets us compare pieces of text in the vector space.
这让我们可以比较矢量空间中的文本片段。

75
00:04:35,280 --> 00:04:38,940
In the example below, we can see that we have three sentences.
在下面的例子中，我们可以看到，我们有三个句子。

76
00:04:38,940 --> 00:04:43,240
The first two are about pets, while the third is about a car.
前两个是关于宠物的，而第三个是关于汽车的。

77
00:04:43,240 --> 00:04:47,380
If we look at the representation in the numeric space, we can see that when we compare the
如果我们看一下数字空间中的表示，我们可以看到，当我们把

78
00:04:47,380 --> 00:04:53,200
two vectors on the pieces of text corresponding to the sentences about pets, they're very
在与关于宠物的句子相对应的文本片段上的两个向量，它们非常

79
00:04:53,200 --> 00:04:54,200
similar.
类似的。

80
00:04:54,200 --> 00:04:58,280
While if we compare it to the one that talks about a car, they're not similar at all.
而如果我们把它与谈论汽车的那个相比，它们一点都不相似。

81
00:04:58,280 --> 00:05:02,520
This will let us easily figure out which pieces of text are like each other, which will be
这将使我们很容易地找出哪些文本是彼此相似的，哪些将是

82
00:05:02,520 --> 00:05:07,120
very useful as we think about which pieces of text we want to include when passing to
非常有用，因为我们要考虑在传递到 "我的 "时，要包括哪些文本。

83
00:05:07,120 --> 00:05:09,920
the language model to answer a question.
语言模型来回答一个问题。

84
00:05:09,920 --> 00:05:13,040
The next component that we're going to cover is the vector database.
我们要介绍的下一个组件是矢量数据库。

85
00:05:13,040 --> 00:05:17,040
A vector database is a way to store these vector representations that we created in
矢量数据库是一种存储这些矢量表示的方式，我们在

86
00:05:17,040 --> 00:05:18,040
the previous step.
上一个步骤。

87
00:05:18,040 --> 00:05:22,520
The way that we create this vector database is we populate it with chunks of text coming
我们创建这个矢量数据库的方法是，我们用大块的文本来填充它。

88
00:05:22,520 --> 00:05:23,760
from incoming documents.
从收到的文件中。

89
00:05:23,760 --> 00:05:29,000
When we get a big incoming document, we're first going to break it up into smaller chunks.
当我们得到一个大的传入文件时，我们首先要把它分解成小块。

90
00:05:29,000 --> 00:05:33,520
This helps create pieces of text that are smaller than the original document, which
这有助于创建比原始文件更小的文本片段，从而

91
00:05:33,520 --> 00:05:37,880
is useful because we may not be able to pass the whole document to the language model.
是有用的，因为我们可能无法将整个文档传递给语言模型。

92
00:05:37,880 --> 00:05:42,000
So we want to create these small chunks so we can only pass the most relevant ones to
因此，我们希望创建这些小块，这样我们就可以只把最相关的块传递给

93
00:05:42,000 --> 00:05:43,300
the language model.
的语言模型。

94
00:05:43,300 --> 00:05:47,480
We then create an embedding for each of these chunks, and then we store those in a vector
然后，我们为这些块中的每一个创建一个嵌入，然后我们将这些存储在一个向量中

95
00:05:47,480 --> 00:05:48,960
database.
数据库。

96
00:05:48,960 --> 00:05:51,640
That's what happens when we create the index.
这就是我们创建索引时的情况。

97
00:05:51,640 --> 00:05:56,000
Now that we've got this index, we can use it during runtime to find the pieces of text
现在我们已经有了这个索引，我们可以在运行时使用它来寻找文本的片段

98
00:05:56,000 --> 00:05:58,440
most relevant to an incoming query.
与传入的查询最相关。

99
00:05:58,440 --> 00:06:02,240
When a query comes in, we first create an embedding for that query.
当一个查询进来时，我们首先为该查询创建一个嵌入。

100
00:06:02,240 --> 00:06:08,000
We then compare it to all the vectors in the vector database, and we pick the n most similar.
然后我们将其与向量数据库中的所有向量进行比较，并挑选出最相似的n个向量。

101
00:06:08,000 --> 00:06:12,280
These are then returned, and we can pass those in the prompt to the language model to get
然后返回这些信息，我们可以将提示中的这些信息传递给语言模型，得到

102
00:06:12,280 --> 00:06:14,380
back a final answer.
回一个最终的答案。

103
00:06:14,380 --> 00:06:17,760
So above, we created this chain and only a few lines of code.
因此，在上面，我们创建了这个链条，只用了几行代码。

104
00:06:17,760 --> 00:06:19,200
That's great for getting started quickly.
这对快速入门很有帮助。

105
00:06:19,200 --> 00:06:23,760
Well, let's now do it a bit more step by step and understand what exactly is going on under
好了，现在让我们再一步步来，了解下到底发生了什么事

106
00:06:23,760 --> 00:06:25,240
the hood.
引擎盖。

107
00:06:25,240 --> 00:06:27,240
The first step is similar to above.
第一步与上面类似。

108
00:06:27,240 --> 00:06:32,400
We're going to create a document loader, loading from that CSV with all the descriptions of
我们将创建一个文件加载器，从CSV中加载所有的描述，包括

109
00:06:32,400 --> 00:06:36,560
the products that we want to do question answering over.
我们想做问题回答的产品。

110
00:06:36,560 --> 00:06:41,480
We can then load documents from this document loader.
然后我们可以从这个文件加载器中加载文件。

111
00:06:41,480 --> 00:06:47,120
If we look at the individual documents, we can see that each document corresponds to
如果我们看一下各个文件，我们可以看到每个文件对应的是

112
00:06:47,120 --> 00:06:49,760
one of the products in the CSV.
CSV中的一个产品。

113
00:06:49,760 --> 00:06:53,080
Previously, we talked about creating chunks.
之前，我们谈到了创建块状物。

114
00:06:53,080 --> 00:06:56,120
Because these documents are already so small, we actually don't need to do any chunking
因为这些文件已经非常小了，我们实际上不需要做任何分块。

115
00:06:56,120 --> 00:06:57,120
here.
这里。

116
00:06:57,120 --> 00:07:01,160
And so we can create embeddings directly.
因此，我们可以直接创建嵌入。

117
00:07:01,160 --> 00:07:05,260
To create embeddings, we're going to use OpenAI's embedding class.
为了创建嵌入，我们将使用OpenAI的嵌入类。

118
00:07:05,260 --> 00:07:08,240
We can import it and initialize it here.
我们可以在这里导入它并初始化它。

119
00:07:08,240 --> 00:07:12,440
If we want to see what these embeddings do, we can actually take a look at what happens
如果我们想看看这些嵌入的作用，我们实际上可以看一下发生了什么

120
00:07:12,440 --> 00:07:21,220
when we embed a particular piece of text.
当我们嵌入一个特定的文本时。

121
00:07:21,220 --> 00:07:25,240
Let's use the embed query method on the embeddings object to create embeddings for a particular
让我们在嵌入对象上使用嵌入查询方法，为一个特定的嵌入对象创建嵌入。

122
00:07:25,240 --> 00:07:26,240
piece of text.
片的文字。

123
00:07:26,240 --> 00:07:28,680
In this case, the sentence, "Hi, my name is Harrison."
在这种情况下，句子，"嗨，我的名字是哈里森。"

124
00:07:28,680 --> 00:07:34,880
If we take a look at this embedding, we can see that there are over a thousand different
如果我们看一下这个嵌入，我们可以看到，有超过一千种不同的

125
00:07:34,880 --> 00:07:42,020
elements.
元素。

126
00:07:42,020 --> 00:07:44,640
Each of these elements is a different numerical value.
这些元素中的每一个都是不同的数值。

127
00:07:44,640 --> 00:07:51,340
Combined, this creates the overall numerical representation for this piece of text.
结合起来，这就形成了这篇文字的整体数字表示。

128
00:07:51,340 --> 00:07:55,940
We want to create embeddings for all the pieces of text that we just loaded, and then we also
我们要为我们刚刚加载的所有文本片段创建嵌入，然后我们还要

129
00:07:55,940 --> 00:07:58,520
want to store them in a vector store.
想把它们储存在一个矢量存储中。

130
00:07:58,520 --> 00:08:03,800
We can do that by using the from documents method on the vector store.
我们可以通过在矢量存储上使用from documents方法来做到这一点。

131
00:08:03,800 --> 00:08:09,080
This method takes in a list of documents, an embedding object, and then we'll create
这个方法接收了一个文档列表，一个嵌入对象，然后我们将创建

132
00:08:09,080 --> 00:08:12,260
an overall vector store.
一个整体的矢量存储。

133
00:08:12,260 --> 00:08:18,000
We can now use this vector store to find pieces of text similar to an incoming query.
我们现在可以使用这个矢量存储来寻找与传入的查询相似的文本片段。

134
00:08:18,000 --> 00:08:23,040
So let's look at the query, "Please suggest a shirt with sunblocking."
因此，让我们来看看这个查询，"请建议一件具有遮阳功能的衬衫"。

135
00:08:23,040 --> 00:08:27,080
If we use the similarity search method on the vector store and pass in a query, we will
如果我们在向量存储上使用相似性搜索方法并传入一个查询，我们将

136
00:08:27,080 --> 00:08:36,860
get back a list of documents.
得到的是一份文件清单。

137
00:08:36,860 --> 00:08:43,960
We can see that it returns four documents, and if we look at the first one, we can see
我们可以看到，它返回了四个文件，如果我们看一下第一个文件，我们可以看到

138
00:08:43,960 --> 00:08:48,040
that it is indeed a shirt about sunblocking.
这的确是一件关于遮阳的衬衫。

139
00:08:48,040 --> 00:08:52,720
So how do we use this to do question answering over our own documents?
那么，我们如何利用这一点对我们自己的文件进行答题？

140
00:08:52,720 --> 00:08:57,120
First, we need to create a retriever from this vector store.
首先，我们需要从这个矢量存储中创建一个检索器。

141
00:08:57,120 --> 00:09:01,480
A retriever is a generic interface that can be underpinned by any method that takes in
检索器是一个通用的接口，它可以由任何方法来支撑，这些方法在

142
00:09:01,480 --> 00:09:03,840
a query and returns documents.
一个查询并返回文件。

143
00:09:03,840 --> 00:09:07,420
Vector stores and embeddings are one such method to do so, although there are plenty
矢量存储和嵌入是这样的一种方法，尽管有很多

144
00:09:07,420 --> 00:09:11,320
of different methods, some less advanced, some more advanced.
不同的方法，有些不太先进，有些比较先进。

145
00:09:11,320 --> 00:09:16,320
Next, because we want to do text generation and return a natural language response, we're
接下来，因为我们想做文本生成并返回自然语言响应，所以我们要

146
00:09:16,320 --> 00:09:20,920
going to import a language model, and we're going to use chat open AI.
要导入一个语言模型，我们要用聊天的方式开放AI。

147
00:09:20,920 --> 00:09:26,760
If we were doing this by hand, what we would do is we would combine the documents into
如果我们用手来做这件事，我们会做的是将文件合并到

148
00:09:26,760 --> 00:09:28,800
a single piece of text.
一个单一的文本。

149
00:09:28,800 --> 00:09:33,640
So we'd do something like this, where we join all the page content in the documents into
因此，我们会做这样的事情，我们把所有文件中的页面内容加入到

150
00:09:33,640 --> 00:09:42,080
a variable, and then we'd pass this variable or a variant on the question, like "Please
一个变量，然后我们将这个变量或问题的一个变体传给他，比如 "请你

151
00:09:42,080 --> 00:09:45,760
list all your shirts with sun protection in a table with markdown," and summarize each
在表格中列出所有具有防晒功能的衬衫，并标明标记"，并对每件衬衫进行总结。

152
00:09:45,760 --> 00:09:49,000
one into the language model.
一个进入语言模型。

153
00:09:49,000 --> 00:09:54,280
And if we print out the response here, we can see that we get back a table exactly as
如果我们在这里打印出响应，我们可以看到，我们得到的是一个表，正好是

154
00:09:54,280 --> 00:09:55,760
we asked for.
我们要求的。

155
00:09:55,760 --> 00:09:59,900
All of those steps can be encapsulated with the LangChain chain.
所有这些步骤都可以用LangChain链进行封装。

156
00:09:59,900 --> 00:10:02,980
So here we can create a retrieval QA chain.
因此，在这里我们可以创建一个检索的QA链。

157
00:10:02,980 --> 00:10:06,880
This does retrieval and then does question answering over the retrieved documents.
这将进行检索，然后对检索到的文件进行问题回答。

158
00:10:06,880 --> 00:10:09,840
To create such a chain, we'll pass in a few different things.
为了创建这样一个链条，我们将传入一些不同的东西。

159
00:10:09,840 --> 00:10:12,200
First, we'll pass in the language model.
首先，我们要传入语言模型。

160
00:10:12,200 --> 00:10:15,280
This will be used for doing the text generation at the end.
这将用于在最后进行文本生成。

161
00:10:15,280 --> 00:10:17,680
Next, we'll pass in the chain type.
接下来，我们将传入链条类型。

162
00:10:17,680 --> 00:10:18,680
We're going to use stuff.
我们要用东西。

163
00:10:18,680 --> 00:10:23,560
This is the simplest method, as it just stuffs all the documents into context and makes one
这是最简单的方法，因为它只是把所有的文件塞到上下文中，并使一个

164
00:10:23,560 --> 00:10:25,360
call to a language model.
对语言模型的调用。

165
00:10:25,360 --> 00:10:29,160
There are a few other methods that you can use to do question answering that I'll maybe
还有一些其他的方法，你可以用来做问题的回答，我也许会

166
00:10:29,160 --> 00:10:31,960
touch on at the end, but we're not going to look at in detail.
在最后触及，但我们不打算详细看。

167
00:10:31,960 --> 00:10:34,440
Third, we're going to pass in a retriever.
第三，我们要传入一个检索器。

168
00:10:34,440 --> 00:10:38,640
The retriever we created above is just an interface for fetching documents.
我们上面创建的检索器只是一个获取文件的接口。

169
00:10:38,640 --> 00:10:41,840
This will be used to fetch documents and pass it to the language model.
这将被用来获取文件并将其传递给语言模型。

170
00:10:41,840 --> 00:10:46,320
And then finally, we're going to set verbose equals to true.
最后，我们要把verbose等效为true。

171
00:10:46,320 --> 00:11:08,440
Now we can create a query and we can run the chain on this query.
现在我们可以创建一个查询，我们可以在这个查询上运行链。

172
00:11:08,440 --> 00:11:14,840
When we get the response, we can again display it using the display and markdown utilities.
当我们得到响应时，我们可以再次使用显示和标记工具来显示它。

173
00:11:14,840 --> 00:11:20,920
You can pause the video here and try it out with a bunch of different queries.
你可以在这里暂停视频，用一堆不同的查询进行尝试。

174
00:11:20,920 --> 00:11:23,920
So that's how you do it in detail, but remember that we can still do it pretty easily with
所以这就是你的详细做法，但记住，我们仍然可以很容易地用

175
00:11:23,920 --> 00:11:26,560
just the one line that we had up above.
只是我们上面的那一行。

176
00:11:26,560 --> 00:11:30,240
So these two things equate to the same result.
所以这两件事相当于同一个结果。

177
00:11:30,240 --> 00:11:32,720
And that's part of the interesting stuff about LinkChain.
而这正是LinkChain的部分有趣之处。

178
00:11:32,720 --> 00:11:35,700
You can do it in one line or you can look at the individual things and break it down
你可以用一条线来做，或者你可以看一下个别的东西，把它分解开来

179
00:11:35,700 --> 00:11:38,160
into five more detailed ones.
变成五个更详细的。

180
00:11:38,160 --> 00:11:43,000
The five more detailed ones let you set more specifics about what exactly is going on,
五个更详细的让你设置更多关于到底发生了什么的具体细节、

181
00:11:43,000 --> 00:11:44,860
but the one-liner is easy to get started.
但单行本很容易上手。

182
00:11:44,860 --> 00:11:48,420
So up to you as to how you'd prefer to go forward.
因此，由你决定你喜欢如何向前走。

183
00:11:48,420 --> 00:11:51,760
We can also customize the index when we're creating it.
我们还可以在创建索引时进行自定义。

184
00:11:51,760 --> 00:11:55,320
And so if you remember, when we created it by hand, we specified an embedding and we
因此，如果你还记得，当我们手工创建它时，我们指定了一个嵌入，我们

185
00:11:55,320 --> 00:11:57,760
can specify an embedding here as well.
也可以在这里指定一个嵌入。

186
00:11:57,760 --> 00:12:01,880
And so this will give us flexibility over how the embeddings themselves are created.
因此，这将使我们在如何创建嵌入本身方面获得灵活性。

187
00:12:01,880 --> 00:12:06,640
And we can also swap out the vector store here for a different type of vector store.
我们还可以把这里的矢量存储换成另一种类型的矢量存储。

188
00:12:06,640 --> 00:12:11,000
So there's the same level of customization that you did when you created by hand that's
因此，有与你手工创建时相同的定制水平，这是

189
00:12:11,000 --> 00:12:15,120
also available when you create the index here.
当你在这里创建索引时也可以使用。

190
00:12:15,120 --> 00:12:16,840
We use the stuff method in this notebook.
我们在这个笔记本中使用了东西法。

191
00:12:16,840 --> 00:12:19,400
The stuff method is really nice because it's pretty simple.
东西法真的很好，因为它很简单。

192
00:12:19,400 --> 00:12:23,760
You just put all of it into one prompt and send that to the language model and get back
你只需把所有的东西放到一个提示中，并把它送到语言模型中，然后得到反馈

193
00:12:23,760 --> 00:12:25,240
one response.
一个反应。

194
00:12:25,240 --> 00:12:27,800
So it's quite simple to understand what's going on.
因此，要理解发生的事情是很简单的。

195
00:12:27,800 --> 00:12:30,520
It's quite cheap and it works pretty well.
它相当便宜，而且效果相当好。

196
00:12:30,520 --> 00:12:32,760
But that doesn't always work okay.
但这并不总是奏效。

197
00:12:32,760 --> 00:12:37,120
So if you remember, when we fetched the documents in the notebook, we only got four documents
因此，如果你还记得，当我们在笔记本上检索文件时，我们只得到四个文件

198
00:12:37,120 --> 00:12:39,360
back and they were relatively small.
他们在后面，而且相对较小。

199
00:12:39,360 --> 00:12:42,960
But what if you wanted to do the same type of question answering over lots of different
但是，如果你想在许多不同的地方做同样类型的问题回答，怎么办？

200
00:12:42,960 --> 00:12:44,680
types of chunks?
块的类型？

201
00:12:44,680 --> 00:12:47,040
Then there are a few different methods that we can use.
然后有一些不同的方法，我们可以使用。

202
00:12:47,040 --> 00:12:48,800
The first is MapReduce.
第一个是MapReduce。

203
00:12:48,800 --> 00:12:53,160
This basically takes all the chunks, passes them along with the question to a language
这基本上是把所有的块，连同问题一起传递给一个语言的

204
00:12:53,160 --> 00:12:58,680
model, gets back a response, and then uses another language model call to summarize all
模型，得到一个响应，然后使用另一个语言模型调用来总结所有的

205
00:12:58,680 --> 00:13:02,440
of the individual responses into a final answer.
将单个的回答变成一个最终的答案。

206
00:13:02,440 --> 00:13:06,880
This is really powerful because it can operate over any number of documents.
这真的很强大，因为它可以在任何数量的文件上操作。

207
00:13:06,880 --> 00:13:11,840
And it's also really powerful because you can do the individual questions in parallel.
而且它也非常强大，因为你可以并行地做各个问题。

208
00:13:11,840 --> 00:13:13,640
But it does take a lot more calls.
但它确实需要更多的电话。

209
00:13:13,640 --> 00:13:19,040
And it does treat all the documents as independent, which may not always be the most desired thing.
而且它确实将所有的文件视为独立的，这可能并不总是最理想的事情。

210
00:13:19,040 --> 00:13:24,220
Refine, which is another method, is again used to loop over many documents.
Refine，这是另一种方法，再次用于循环处理许多文件。

211
00:13:24,220 --> 00:13:25,560
But it actually does it iteratively.
但它实际上是反复进行的。

212
00:13:25,560 --> 00:13:28,740
It builds upon the answer from the previous document.
它建立在前一份文件的答案之上。

213
00:13:28,740 --> 00:13:33,880
So this is really good for combining information and building up an answer over time.
因此，这对于结合信息并随着时间的推移建立起一个答案真的很好。

214
00:13:33,880 --> 00:13:36,560
It will generally lead to longer answers.
这通常会导致更长的答案。

215
00:13:36,560 --> 00:13:39,760
And it's also not as fast because now the calls aren't independent.
而且它也没有那么快，因为现在的呼叫不是独立的。

216
00:13:39,760 --> 00:13:43,360
They depend on the result of previous calls.
它们取决于以前调用的结果。

217
00:13:43,360 --> 00:13:48,880
This means that it often takes a good while longer and takes just as many calls as MapReduce,
这意味着它往往需要更长的时间，并且需要和MapReduce一样多的调用、

218
00:13:48,880 --> 00:13:49,960
basically.
基本上。

219
00:13:49,960 --> 00:13:55,040
MapRerank is a pretty interesting and a bit more experimental one, where you do a single
MapRerank是一个相当有趣的，也是一个有点实验性的，你在这里做一个单一的

220
00:13:55,040 --> 00:13:57,980
call to the language model for each document.
对每个文件的语言模型的调用。

221
00:13:57,980 --> 00:14:00,300
And you also ask it to return a score.
而且你还要求它返回一个分数。

222
00:14:00,300 --> 00:14:02,640
And then you select the highest score.
然后你选择最高分。

223
00:14:02,640 --> 00:14:06,220
This relies on the language model to know what the score should be.
这要依靠语言模型来知道分数应该是多少。

224
00:14:06,220 --> 00:14:09,960
So you often have to tell it, hey, it should be a high score if it's relevant to the document
所以你经常要告诉它，嘿，如果它与文件相关，就应该是一个高分。

225
00:14:09,960 --> 00:14:12,720
and really refine the instructions there.
并真正完善那里的指示。

226
00:14:12,720 --> 00:14:15,120
Similar to MapReduce, all the calls are independent.
与MapReduce类似，所有的调用都是独立的。

227
00:14:15,120 --> 00:14:16,360
So you can batch them.
所以你可以分批进行。

228
00:14:16,360 --> 00:14:18,200
And it's relatively fast.
而且，它的速度相对较快。

229
00:14:18,200 --> 00:14:20,780
But again, you're making a bunch of language model calls.
但同样，你在做一堆语言模型的调用。

230
00:14:20,780 --> 00:14:22,720
So it will be a bit more expensive.
所以它将会更贵一些。

231
00:14:22,720 --> 00:14:27,000
The most common of these methods is the stuff method, which we used in the notebook to combine
这些方法中最常见的是东西法，我们在笔记本中用它来组合

232
00:14:27,000 --> 00:14:29,200
it all into one document.
这一切都在一个文件中。

233
00:14:29,200 --> 00:14:33,640
The second most common is the MapReduce method, which takes these chunks and sends them to
第二种最常见的是MapReduce方法，它将这些分块发送到

234
00:14:33,640 --> 00:14:35,560
the language model.
的语言模型。

235
00:14:35,560 --> 00:14:39,960
These methods here-- stuff, MapReduce, refine, and rerank-- can also be used for lots of
这里的这些方法--stuff、MapReduce、refine和rerank--也可用于很多的

236
00:14:39,960 --> 00:14:42,740
other chains besides just question answering.
除了回答问题外，还有其他连锁店。

237
00:14:42,740 --> 00:14:47,800
For example, a really common use case of the MapReduce chain is for summarization, where
例如，MapReduce链的一个真正常见的用例是用于总结，其中

238
00:14:47,800 --> 00:14:49,640
you have a really long document.
你有一个非常长的文件。

239
00:14:49,640 --> 00:14:53,800
And you want to recursively summarize pieces of information in it.
而你想递归地总结其中的信息片段。

240
00:14:53,800 --> 00:14:56,180
That's it for question answering over documents.
对文件的问题回答就到此为止。

241
00:14:56,180 --> 00:15:00,580
As you may have noticed, there's a lot going on in the different chains that we have here.
正如你可能已经注意到的，我们这里的不同连锁店有很多事情要做。

242
00:15:00,580 --> 00:15:04,400
And so in the next section, we'll cover ways to better understand what exactly is going
因此，在下一节中，我们将介绍如何更好地了解究竟发生了什么事。

243
00:15:04,400 --> 00:15:06,240
on inside all of these chains.
在所有这些连锁店内。

